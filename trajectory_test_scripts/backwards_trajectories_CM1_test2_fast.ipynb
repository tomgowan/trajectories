{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "In recent years, Cloud Model 1 (CM1; http://www2.mmm.ucar.edu/people/bryan/cm1/) has become a very popular tool for performing idealized studies of atmospheric phenomena. There exists very little support for computing trajectories using CM1 output, which are usually necessary to understand the processes of the atmospheric phenomena of interest. Natively, CM1 only supports 'online' forward trajectories in 2D simulations and in 3D simulation without terrain. I wrote this script because there are no adequate tools available to compute highly customizable 'offline' trajectories in simulations with terrain. This script is intended to be easily customizable.\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Can compute backward or forward trajectories (Default is backward, but can be forward with simple changes to \"Calculate Trajectories\" block)\n",
    "* Written to work with 3D model output (can be modified to work with 2D output)\n",
    "* Will work with or without terrain\n",
    "* Initial location, number, and density of parcels can be easily specified in \"Initialize Parcels\" block\n",
    "* Uses xarray and Dask to distribute memory and calculation across multiple processors\n",
    "* With modifications, can be used with WRF output (several others have already done so)\n",
    "* Comments that say \"set by user\" are specific to model output and desired trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scipy import interpolate\n",
    "import time\n",
    "\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in CM1 Output\n",
    "\n",
    "* User must insert path to data\n",
    "    * If model output is one file use ***xr.open_dataset***\n",
    "    * If model output is in multiple files use ***xr.openmfdataset***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use xarray to open model output and specify chunking if data set is large (set by user)\n",
    "ds = xr.open_dataset('/uufs/chpc.utah.edu/common/home/steenburgh-group8/tom/cm1/output/15ms_1500m_tug.nc')\n",
    "\n",
    "#Get model output dimensions\n",
    "num_x = ds.nx #Number of gridpoints in x\n",
    "num_y = ds.ny #Number of gridpoints in y\n",
    "num_z = ds.nz #Number of gridpoints in z\n",
    "\n",
    "x = np.arange(0,num_x,1)\n",
    "y = np.arange(0,num_y,1)\n",
    "z = np.arange(0,num_z,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dask Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:33498\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:34861/status' target='_blank'>http://127.0.0.1:34861/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>10</li>\n",
       "  <li><b>Cores: </b>40</li>\n",
       "  <li><b>Memory: </b>96.49 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:33498' processes=10 cores=40>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Option to use multiple processors and threads (set by user)\n",
    "from dask.distributed import Client, LocalCluster\n",
    "c = LocalCluster(n_workers=10, threads_per_worker=4)\n",
    "client = Client(c)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Parcels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User must enter desired trajectory characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of parcels in vertical (can be more than number of vertical levels; set by user) \n",
    "num_seeds_z = 1\n",
    "\n",
    "#Number of parcels in y (set by user) \n",
    "num_seeds_y = 50\n",
    "\n",
    "#Number of time steps to run trajectories back (set by user) \n",
    "time_steps = 130\n",
    "\n",
    "#Time step to start backward trajectories at (set by user) \n",
    "start_time_step = 240\n",
    "\n",
    "#Variable to record at each parcel's location throughout trajectory (code can be easily modified to add more; set by user) \n",
    "var_name1 = 'th'\n",
    "\n",
    "#Set as 'Y' or 'N' for 'yes' or 'no' if the u, v, and w model output is on the staggered grid \n",
    "#(unless you have interpolated u, v, and w to the scalar grid, they are most likely on the staggered grid (set by user)\n",
    "staggered = 'N'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model output info**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Horizontal resolution of model output (meters)\n",
    "hor_resolution = (ds.xf[1].values-ds.xf[0].values)*1000\n",
    "\n",
    "#Vertical resolution of model output (meters). Changes in x and y, if there is terrain, and z, if grid is stretched.\n",
    "vert_resolution = ds.zh[0,1:,:,:].values-ds.zh[0,:-1,:,:].values \n",
    "                  \n",
    "#Model output time step length (seconds)\n",
    "time_step_length = (ds.time[1].values - ds.time[0].values)/np.timedelta64(1, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create empty arrays to store x, y, and z positions of parcels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpos = np.zeros((time_steps, num_seeds_z, num_seeds_y)) #x-location (grid points on staggered grid)\n",
    "ypos = np.zeros((time_steps, num_seeds_z, num_seeds_y)) #y-location (grid points on staggered grid)\n",
    "zpos = np.zeros((time_steps, num_seeds_z, num_seeds_y)) #z-location (grid points on staggered grid)\n",
    "zpos_heightASL = np.zeros((time_steps, num_seeds_z, num_seeds_y)) #Height above sea level (meters)\n",
    "zpos_vert_res = np.zeros((time_steps, num_seeds_z, num_seeds_y)) #Vertical grid spacing at parcel location (meters)\n",
    "variable1 = np.zeros((time_steps, num_seeds_z, num_seeds_y)) #User specified variable to track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial location of parcels in gridpoints, specifically on the staggered grid (set by user). Initializes an array of parcels in the the y-z domain (modification necessary for x-dimension or 3D array of parcels)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x-position\n",
    "xpos[0,:,:] = 2900 #This example initializes all seeds at same x-position (1000th x-grpt, set by user)\n",
    "\n",
    "#y-position   \n",
    "for i in range(num_seeds_y):\n",
    "    ypos[0,:,i] = 350+(4*i) #This example initializes seeds evenly in y-dimension (0th, 4th, 8th, etc. y-grpt; set by user)\n",
    "\n",
    "#z-position\n",
    "for i in range(num_seeds_z):\n",
    "    zpos[0,i,:] = 2 #This example initializes seeds evenly starting in z-dimension (0th, 1st, 2nd, etc., z-grpt; set by user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Initial Height of Parcels Above Sea Level\n",
    "Use the height of the models levels (meters above sea level) to convert from terrain following grid points to height above seal level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get height of vertical coordinates (scalar grid)\n",
    "zh = ds.zh[0,:,:,:].values\n",
    "\n",
    "#Create list of initial coordinates to get height (must subtract 0.5 grdpt from each location because zh is on scalar grid)\n",
    "xloc = (xpos[0,:,:]-0.5).flatten()\n",
    "yloc = (ypos[0,:,:]-0.5).flatten()\n",
    "zloc = (zpos[0,:,:]-0.5).flatten()\n",
    "coord_height = []\n",
    "for i in range(len(xloc)):\n",
    "    coord_height.append((zloc[i], yloc[i], xloc[i]))\n",
    "\n",
    "#Get the actual inital height of the parcels in meters above sea level\n",
    "zpos_heightASL[0,:,:] = np.reshape(interpolate.interpn((z,y,x), zh, coord_height, method='linear', bounds_error=False, fill_value= 0), (num_seeds_z, num_seeds_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Trajectories\n",
    "Unless user is changing trajectories from backwards to forwards, nothing should be changed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integration 0 took 1.08 seconds\n",
      "Integration 1 took 1.59 seconds\n",
      "Integration 2 took 2.85 seconds\n",
      "Integration 3 took 2.41 seconds\n",
      "Integration 4 took 3.08 seconds\n",
      "Integration 5 took 3.11 seconds\n",
      "Integration 6 took 3.30 seconds\n",
      "Integration 7 took 3.39 seconds\n",
      "Integration 8 took 3.95 seconds\n",
      "Integration 9 took 3.66 seconds\n",
      "Integration 10 took 3.30 seconds\n",
      "Integration 11 took 2.75 seconds\n",
      "Integration 12 took 3.12 seconds\n",
      "Integration 13 took 3.69 seconds\n",
      "Integration 14 took 3.11 seconds\n",
      "Integration 15 took 3.44 seconds\n",
      "Integration 16 took 3.46 seconds\n",
      "Integration 17 took 3.42 seconds\n",
      "Integration 18 took 3.40 seconds\n",
      "Integration 19 took 3.18 seconds\n",
      "Integration 20 took 3.20 seconds\n",
      "Integration 21 took 3.19 seconds\n",
      "Integration 22 took 3.47 seconds\n",
      "Integration 23 took 3.45 seconds\n",
      "Integration 24 took 3.42 seconds\n",
      "Integration 25 took 3.42 seconds\n",
      "Integration 26 took 3.40 seconds\n",
      "Integration 27 took 3.60 seconds\n",
      "Integration 28 took 3.97 seconds\n",
      "Integration 29 took 3.73 seconds\n",
      "Integration 30 took 4.00 seconds\n",
      "Integration 31 took 4.67 seconds\n",
      "Integration 32 took 3.95 seconds\n",
      "Integration 33 took 3.40 seconds\n",
      "Integration 34 took 3.71 seconds\n",
      "Integration 35 took 4.29 seconds\n",
      "Integration 36 took 4.10 seconds\n",
      "Integration 37 took 4.43 seconds\n",
      "Integration 38 took 4.63 seconds\n",
      "Integration 39 took 4.90 seconds\n",
      "Integration 40 took 5.15 seconds\n",
      "Integration 41 took 4.98 seconds\n",
      "Integration 42 took 5.46 seconds\n",
      "Integration 43 took 5.37 seconds\n",
      "Integration 44 took 4.52 seconds\n",
      "Integration 45 took 3.95 seconds\n",
      "Integration 46 took 3.50 seconds\n",
      "Integration 47 took 3.98 seconds\n",
      "Integration 48 took 3.84 seconds\n",
      "Integration 49 took 4.11 seconds\n",
      "Integration 50 took 4.18 seconds\n",
      "Integration 51 took 4.37 seconds\n",
      "Integration 52 took 4.36 seconds\n",
      "Integration 53 took 4.51 seconds\n",
      "Integration 54 took 4.55 seconds\n",
      "Integration 55 took 4.51 seconds\n",
      "Integration 56 took 4.41 seconds\n",
      "Integration 57 took 4.21 seconds\n",
      "Integration 58 took 4.24 seconds\n",
      "Integration 59 took 4.23 seconds\n",
      "Integration 60 took 4.26 seconds\n",
      "Integration 61 took 4.23 seconds\n",
      "Integration 62 took 4.19 seconds\n",
      "Integration 63 took 4.38 seconds\n",
      "Integration 64 took 4.38 seconds\n",
      "Integration 65 took 4.46 seconds\n",
      "Integration 66 took 4.47 seconds\n",
      "Integration 67 took 4.43 seconds\n",
      "Integration 68 took 4.47 seconds\n",
      "Integration 69 took 4.55 seconds\n",
      "Integration 70 took 4.55 seconds\n",
      "Integration 71 took 4.57 seconds\n",
      "Integration 72 took 4.56 seconds\n",
      "Integration 73 took 4.69 seconds\n",
      "Integration 74 took 5.20 seconds\n",
      "Integration 75 took 5.36 seconds\n",
      "Integration 76 took 5.27 seconds\n",
      "Integration 77 took 5.73 seconds\n",
      "Integration 78 took 5.68 seconds\n",
      "Integration 79 took 5.40 seconds\n",
      "Integration 80 took 5.43 seconds\n",
      "Integration 81 took 5.38 seconds\n",
      "Integration 82 took 5.61 seconds\n",
      "Integration 83 took 5.59 seconds\n",
      "Integration 84 took 5.66 seconds\n",
      "Integration 85 took 5.62 seconds\n",
      "Integration 86 took 5.64 seconds\n",
      "Integration 87 took 5.62 seconds\n",
      "Integration 88 took 5.82 seconds\n",
      "Integration 89 took 5.85 seconds\n",
      "Integration 90 took 5.82 seconds\n",
      "Integration 91 took 5.62 seconds\n",
      "Integration 92 took 5.57 seconds\n",
      "Integration 93 took 5.57 seconds\n",
      "Integration 94 took 5.83 seconds\n",
      "Integration 95 took 5.79 seconds\n",
      "Integration 96 took 5.31 seconds\n",
      "Integration 97 took 5.33 seconds\n",
      "Integration 98 took 5.67 seconds\n",
      "Integration 99 took 5.50 seconds\n",
      "Integration 100 took 5.60 seconds\n",
      "Integration 101 took 5.53 seconds\n",
      "Integration 102 took 5.59 seconds\n",
      "Integration 103 took 5.58 seconds\n",
      "Integration 104 took 5.61 seconds\n",
      "Integration 105 took 5.58 seconds\n",
      "Integration 106 took 5.58 seconds\n",
      "Integration 107 took 5.58 seconds\n",
      "Integration 108 took 5.36 seconds\n",
      "Integration 109 took 5.32 seconds\n",
      "Integration 110 took 4.88 seconds\n",
      "Integration 111 took 4.77 seconds\n",
      "Integration 112 took 4.87 seconds\n",
      "Integration 113 took 4.81 seconds\n",
      "Integration 114 took 4.16 seconds\n",
      "Integration 115 took 5.03 seconds\n",
      "Integration 116 took 4.52 seconds\n",
      "Integration 117 took 4.62 seconds\n",
      "Integration 118 took 5.63 seconds\n",
      "Integration 119 took 5.66 seconds\n",
      "Integration 120 took 5.58 seconds\n",
      "Integration 121 took 5.57 seconds\n",
      "Integration 122 took 5.59 seconds\n",
      "Integration 123 took 5.34 seconds\n",
      "Integration 124 took 4.07 seconds\n",
      "Integration 125 took 3.84 seconds\n",
      "Integration 126 took 3.56 seconds\n",
      "Integration 127 took 3.38 seconds\n",
      "Integration 128 took 1.70 seconds\n"
     ]
    }
   ],
   "source": [
    "#Loop over all time steps and compute trajectory\n",
    "for t in range(time_steps-1):\n",
    "    \n",
    "    start = time.time() #Timer\n",
    "    \n",
    "    xmin = np.int(np.nanmin(xpos[t,:,:])-2)\n",
    "    xmin = 0 if xmin < 0 else xmin\n",
    "    \n",
    "    xmax = np.int(np.nanmax(xpos[t,:,:])+2)\n",
    "    xmax = ds.nx if xmax > ds.nx else xmax\n",
    "    \n",
    "    ymin = np.int(np.nanmin(ypos[t,:,:])-2)\n",
    "    ymin = 0 if ymin < 0 else ymin\n",
    "    \n",
    "    ymax = np.int(np.nanmax(ypos[t,:,:])+2)\n",
    "    ymax = ds.ny if ymax > ds.ny else ymax\n",
    "    \n",
    "    zmin = np.int(np.nanmin(zpos[t,:,:])-2)\n",
    "    zmin = 0 if zmin < 0 else zmin\n",
    "    \n",
    "    zmax = np.int(np.nanmax(zpos[t,:,:])+2)\n",
    "    zmax = ds.nz if zmax > ds.nz else zmax\n",
    "    \n",
    "    x_fast = np.arange(0,xmax-xmin)\n",
    "    y_fast = np.arange(0,ymax-ymin)\n",
    "    z_fast = np.arange(0,zmax-zmin)\n",
    "    \n",
    "    #Get model data\n",
    "    u = ds.uinterp[start_time_step-t,zmin:zmax,ymin:ymax,xmin:xmax].values\n",
    "    v = ds.vinterp[start_time_step-t,zmin:zmax,ymin:ymax,xmin:xmax].values\n",
    "    w = ds.winterp[start_time_step-t,zmin:zmax,ymin:ymax,xmin:xmax].values\n",
    "    var1 = getattr(ds,var_name1)[start_time_step-t,zmin:zmax,ymin:ymax,xmin:xmax].values\n",
    "        \n",
    "    #Get surface height grid (set to zero if no terrain)\n",
    "    try:\n",
    "        zs = np.array(ds.zs[0,:,:])\n",
    "    except:\n",
    "        zs = np.zeros((ds.ny, ds.nx))  \n",
    "        \n",
    "        \n",
    "    ############## Generate coordinates for interpolations ###############\n",
    "\n",
    "    #x, y, and z on staggered and scalar grids\n",
    "    xloc = np.copy(xpos[t,:,:]-0.5).flatten()-xmin\n",
    "    xloc_stag = np.copy(xpos[t,:,:]).flatten()-xmin\n",
    "    yloc = np.copy(ypos[t,:,:]-0.5).flatten()-ymin\n",
    "    yloc_stag = np.copy(ypos[t,:,:]).flatten()-ymin\n",
    "    zloc = np.copy(zpos[t,:,:]-0.5).flatten()-zmin\n",
    "    zloc_stag = np.copy(zpos[t,:,:]).flatten()-zmin\n",
    "\n",
    "    #If u, v, and w are staggered, generate three staggered sets of coordinates:\n",
    "    #    1) u-grid (staggered in x)\n",
    "    #    2) v-grid (staggered in y)\n",
    "    #    3) w-grid (staggered in z)\n",
    "    \n",
    "    if staggered == 'Y':\n",
    "        coord_u = []\n",
    "        coord_v = []\n",
    "        coord_w = []\n",
    "        for i in range(len(xloc)):\n",
    "            coord_u.append((zloc[i], yloc[i], xloc_stag[i])) \n",
    "            coord_v.append((zloc[i], yloc_stag[i], xloc[i])) \n",
    "            coord_w.append((zloc_stag[i], yloc[i], xloc[i])) \n",
    "    \n",
    "    #If not, generate scalar coordinates\n",
    "    else: \n",
    "        coord_u = []\n",
    "        coord_v = []\n",
    "        coord_w = []\n",
    "        for i in range(len(xloc)):\n",
    "            coord_u.append((zloc[i], yloc[i], xloc[i])) \n",
    "            coord_v.append((zloc[i], yloc[i], xloc[i])) \n",
    "            coord_w.append((zloc[i], yloc[i], xloc[i])) \n",
    "    \n",
    "    #Scalar coordinates for all other variables\n",
    "    coord = []\n",
    "    coord_fast = []\n",
    "    for i in range(len(xloc)):\n",
    "        coord.append((zloc[i]+zmin, yloc[i]+ymin, xloc[i]+xmin)) \n",
    "        coord_fast.append((zloc[i], yloc[i], xloc[i])) \n",
    "    \n",
    "    ##########################################################################################################   \n",
    "    ########################## Integrate to determine parcel's new location ##################################\n",
    "    ##########################################################################################################   \n",
    "\n",
    "    \n",
    "    #########################   Calc new xpos in grdpts above surface  #######################################\n",
    "    xpos[t+1,:,:] = xpos[t,:,:] - np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), u, coord_u, method='linear', bounds_error=False, fill_value=np.nan)*time_step_length/hor_resolution, (num_seeds_z, num_seeds_y))\n",
    "\n",
    "    #########################   Calc new ypos in grdpts above surface  #######################################\n",
    "    ypos[t+1,:,:]  = ypos[t,:,:] - np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), v, coord_v, method='linear', bounds_error=False, fill_value=np.nan)*time_step_length/hor_resolution, (num_seeds_z, num_seeds_y))\n",
    "\n",
    "    #########################   Calc new zpos in meters above sea level ######################################\n",
    "    zpos_heightASL[t+1,:,:]  = zpos_heightASL[t,:,:] - np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), w, coord_w, method='linear', bounds_error=False, fill_value= 0)*time_step_length, (num_seeds_z, num_seeds_y))\n",
    "\n",
    "    ############# Convert zpos from meters above sea level to gridpts abve surface for interpolation #########\n",
    "    #Get vertical grid spacing at each parcel's location\n",
    "    zpos_vert_res[t,:,:] = np.reshape(interpolate.interpn((z[:-1],y,x), vert_resolution, coord, method='linear', bounds_error=False, fill_value= np.nan), (num_seeds_z, num_seeds_y))\n",
    "\n",
    "    #Get surface height at each parcel's location (scalar)\n",
    "    xloc = np.copy(xpos[t+1,:,:]-0.5).flatten()\n",
    "    yloc = np.copy(ypos[t+1,:,:]-0.5).flatten()\n",
    "    coord_zs = []\n",
    "    for i in range(len(xloc)):\n",
    "        coord_zs.append((yloc[i], xloc[i]))\n",
    "    surface_height = np.reshape(interpolate.interpn((y,x), zs, coord_zs, method='linear', bounds_error=False, fill_value= np.nan), (num_seeds_z, num_seeds_y))\n",
    "    \n",
    "    #Calculate zpos in grdpts above surface\n",
    "    zpos[t+1,:,:] = np.reshape((zpos_heightASL[t+1,:,:].flatten()-surface_height.flatten())/zpos_vert_res[t,:,:].flatten(), (num_seeds_z, num_seeds_y))\n",
    "    \n",
    "    ##########################################################################################################\n",
    "    \n",
    "    \n",
    "    #Prevent parcels from going into the ground\n",
    "    if staggered == 'Y':\n",
    "        zpos = zpos.clip(min=0.5)\n",
    "    else:\n",
    "        zpos = zpos.clip(min=0)\n",
    "    \n",
    "    #Calculate value of variable at each parcel's location\n",
    "    variable1[t,:,:] = np.reshape(interpolate.interpn((z_fast,y_fast,x_fast), var1, coord_fast, method = 'linear', bounds_error=False, fill_value= np.nan), (num_seeds_z, num_seeds_y))  \n",
    "    \n",
    "    #Timer\n",
    "    stop = time.time()\n",
    "    print(\"Integration {:01d} took {:.2f} seconds\".format(t, stop-start))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get variable data for final time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time_steps-1\n",
    "var1 = getattr(ds,var_name1)[start_time_step-t,:,:,:].values\n",
    "\n",
    "#Get get x, y, and z positions from scalar grid\n",
    "xloc = np.copy(xpos[t,:,:]-0.5).flatten()\n",
    "yloc = np.copy(ypos[t,:,:]-0.5).flatten()\n",
    "zloc = np.copy(zpos[t,:,:]-0.5).flatten()\n",
    "coord = []\n",
    "for i in range(len(xloc)):\n",
    "    coord.append((zloc[i], yloc[i], xloc[i])) \n",
    "\n",
    "#Variables\n",
    "variable1[t,:,:] = np.reshape(interpolate.interpn((z,y,x), var1, coord, method = 'linear', bounds_error=False, fill_value= np.nan), (num_seeds_z, num_seeds_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Trajectory Data\n",
    "The x, y, and z positions and user-specified variable values are saved in 3D numpy arrays. The first dimension is time and the other two are the positions and values of variables of all the parcels at that specifc time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('trajectory_arrays/xpos_fast', xpos)\n",
    "np.save('trajectory_arrays/ypos_fast', ypos)\n",
    "np.save('trajectory_arrays/zpos_fast', zpos_heightASL)\n",
    "np.save('trajectory_arrays/%s_fast' %var_name1, variable1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
